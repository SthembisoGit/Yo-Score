<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>YoScore Outcome 7 Academic Submission</title>
  <link rel="stylesheet" href="./styles.css" />
</head>
<body>
  <header class="document-header">
    <h1>YoScore: AI-Aware Developer Trust and Skill Scoring Platform</h1>
    <p class="subtitle">Outcome 7: Industry Project (IP) - Business Analysis and Project Management</p>
    <table class="meta-table">
      <tr><th>Student Name</th><td>Sthembiso Sotsha Ndlovu</td></tr>
      <tr><th>Student Number</th><td>224834659</td></tr>
      <tr><th>Qualification</th><td>Diploma in Computer Science (DPRS20)</td></tr>
      <tr><th>Module</th><td>WOC316D - Work-Integrated Learning</td></tr>
      <tr><th>WIL Period</th><td>01 Sep 2025 - 28 Feb 2026</td></tr>
    </table>
  </header>

  <nav class="toc" aria-label="Table of contents">
    <h2>Table of Contents</h2>
    <ol>
      <li><a href="#phase-1">Phase 1: Proposal</a></li>
      <li><a href="#phase-2">Phase 2: Modelling with Classes</a></li>
      <li><a href="#phase-3">Phase 3: User Interface</a></li>
      <li><a href="#phase-4">Phase 4: Build the Database and Demonstrate Integration</a></li>
      <li><a href="#phase-5">Phase 5: Final Project Deliverance</a></li>
      <li><a href="#appendix">Appendix: Evidence and Source Code Proof</a></li>
    </ol>
  </nav>

  <main>
    <section id="phase-1" class="phase page-break">
      <h2>Phase 1: Proposal</h2>

      <h3>1. Name of the Project</h3>
      <p><strong>YoScore: AI-Aware Developer Trust and Skill Scoring Platform</strong></p>

      <h3>2. Domain Analysis</h3>
      <h4>2.1 General Field of Business</h4>
      <p>
        YoScore operates in software development assessment and technical trust validation.
        The system supports decisions for recruitment, internship placement, and competency development.
      </p>

      <h4>2.2 Terminology and Glossary</h4>
      <ul>
        <li><strong>Challenge</strong>: timed technical task assigned to a user.</li>
        <li><strong>Category</strong>: assessment track such as frontend, backend, cloud, or security.</li>
        <li><strong>Seniority Band</strong>: level derived from work-experience duration.</li>
        <li><strong>Submission</strong>: code sent for automatic evaluation.</li>
        <li><strong>Judge Run</strong>: execution record against test cases.</li>
        <li><strong>Baseline</strong>: language runtime and memory reference for scoring.</li>
        <li><strong>Proctoring Session</strong>: monitored challenge attempt.</li>
        <li><strong>Violation</strong>: suspicious behavior recorded during a session.</li>
        <li><strong>AI Coach</strong>: limited concept-based helper.</li>
        <li><strong>Trust Score</strong>: combined assessment confidence output.</li>
      </ul>

      <h4>2.3 General Knowledge and Business Environment</h4>
      <p>
        AI coding tools are widely used in software teams. Assessment systems must therefore verify genuine understanding,
        not only final answers. Employers and academic evaluators require auditable evidence of technical competence and conduct.
      </p>

      <h4>2.4 Tasks and Procedures Currently Performed</h4>
      <ul>
        <li>candidates take coding tests on different platforms,</li>
        <li>results are often interpreted manually,</li>
        <li>integrity controls vary significantly,</li>
        <li>work experience is frequently self-reported without structured evidence.</li>
      </ul>

      <h4>2.5 Customers and Users</h4>
      <ul>
        <li>Developers and students completing assessments.</li>
        <li>Administrators managing content, quality, and risk controls.</li>
        <li>Recruiters and evaluators consuming report outputs.</li>
      </ul>

      <h4>2.6 Competing Software</h4>
      <ul>
        <li>HackerRank</li>
        <li>Codility</li>
        <li>CodeSignal</li>
        <li>TestGorilla</li>
      </ul>

      <h4>2.7 Similarities to Other Domains</h4>
      <p>
        The domain overlaps with online examination, e-learning, and compliance systems where integrity,
        reliability, and traceable evidence are mandatory.
      </p>

      <h3>3. Define the Problem</h3>
      <p>
        Current technical assessments can produce scores without proving whether the candidate actually understands
        the submitted solution process. This creates risk in interview and hiring decisions.
      </p>
      <p>
        The project opportunity is to provide a three-tier, auditable, and automated assessment system
        that combines coding performance, session behavior, and trust evidence.
      </p>

      <h3>4. Define the Scope</h3>
      <h4>4.1 Scope Boundary</h4>
      <p>Implemented scope includes:</p>
      <ul>
        <li>category and seniority aware challenge assignment,</li>
        <li>timed proctored challenge sessions,</li>
        <li>automated submission judging with per-test persistence,</li>
        <li>AI helper with bounded policy controls,</li>
        <li>work-experience evidence and risk workflow,</li>
        <li>developer and admin dashboards with reporting outputs.</li>
      </ul>

      <h4>4.2 Integrated Result Based Management (IRBM)</h4>
      <p><strong>Inputs</strong>: users, challenges, tests, baselines, proctoring settings, evidence links, infrastructure.</p>
      <p><strong>Activities</strong>: authenticate, assign, monitor, evaluate, score, report, audit.</p>
      <p><strong>Outputs</strong>: scored runs, trust values, violation logs, and risk reports.</p>
      <p><strong>Outcomes</strong>: consistent evidence-based assessment decisions.</p>
      <p><strong>Impact</strong>: improved fairness and confidence in technical evaluation.</p>

      <h4>4.3 Assess / Think / Envision / Plan</h4>
      <p><strong>Assess</strong>: assessment credibility is inconsistent across available processes.</p>
      <p><strong>Think</strong>: root causes include weak integrity controls and non-standard evidence outputs.</p>
      <p><strong>Envision</strong>: a trusted, auditable, and scalable assessment platform.</p>
      <p><strong>Plan</strong>: design, implement, test, and present the solution during the six-month WIL period.</p>

      <h3>5. Vision and Objectives</h3>
      <h4>5.1 Vision</h4>
      <p>
        To provide reliable AI-aware developer assessment that measures both correctness and assessment integrity.
      </p>

      <h4>5.2 SMART Objectives</h4>
      <ol>
        <li>Ensure all submissions are judged and persisted with run/test evidence.</li>
        <li>Enforce timed challenge sessions with server-validated deadline checks.</li>
        <li>Enforce AI helper usage limits for each challenge session.</li>
        <li>Route challenge assignment using category and seniority rules.</li>
        <li>Record experience evidence and verification status for trust reporting.</li>
        <li>Generate role-specific dashboards and report queries for admins and evaluators.</li>
      </ol>

      <h3>6. Users of the System</h3>
      <ul>
        <li><strong>Developer</strong>: solve challenges, submit solutions, and review results.</li>
        <li><strong>Admin</strong>: manage content, publishing, proctoring rules, and audits.</li>
        <li><strong>Recruiter/Evaluator</strong>: consume trust and performance reports.</li>
      </ul>

      <h3>7. Mandatory Functions</h3>
      <ul>
        <li>Add/Register user and challenge data.</li>
        <li>Delete/Remove challenge and related data where allowed by policy.</li>
        <li>Update challenges, tests, baselines, profiles, work-experience, and settings.</li>
      </ul>

      <h3>8. Functional Requirements</h3>
      <ol>
        <li>support registration/login/logout with role protection,</li>
        <li>accept category-based challenge requests and return valid assignment,</li>
        <li>start proctoring sessions and maintain session status,</li>
        <li>accept and evaluate submissions asynchronously,</li>
        <li>persist run-level and test-level outcomes,</li>
        <li>apply scoring and trust aggregation rules,</li>
        <li>accept and validate work-experience evidence data,</li>
        <li>produce dashboard and report outputs for decision support.</li>
      </ol>
      <p><strong>Inputs</strong>: credentials, category, code, language, session events, evidence links.</p>
      <p><strong>Outputs</strong>: judged results, score components, trust metrics, report views.</p>
      <p><strong>Computations</strong>: correctness, efficiency, style contribution, risk/trust aggregation.</p>
      <p><strong>Timing/Synchronization</strong>: queue-driven evaluation, polling updates, and deadline enforcement.</p>

      <h3>9. Non-functional Requirements</h3>
      <ul>
        <li><strong>Authentication</strong>: secure login/logout and protected route access.</li>
        <li><strong>Availability</strong>: health endpoints and resilient service orchestration.</li>
        <li><strong>Security</strong>: role checks, password hashing, controlled CORS.</li>
        <li><strong>Reliability</strong>: persistent records for submissions, sessions, and audits.</li>
        <li><strong>Maintainability</strong>: modular backend services and documented contracts.</li>
      </ul>

      <h3>10. Use Case</h3>
      <figure>
        <img src="./assets/use-case.png" alt="Use case diagram" />
        <figcaption>Figure 1: Use Case Diagram</figcaption>
      </figure>
      <p>
        The primary user interaction sequence includes challenge request, session start,
        monitored attempt, submission, automated evaluation, and result publication.
      </p>

      <h3>11. Tools and Technologies to be Used</h3>
      <ul>
        <li><strong>Frontend</strong>: React, TypeScript, Vite.</li>
        <li><strong>Backend</strong>: Node.js, Express, TypeScript.</li>
        <li><strong>Queue and Cache</strong>: BullMQ, Redis.</li>
        <li><strong>Database</strong>: PostgreSQL.</li>
        <li><strong>Proctoring Service</strong>: FastAPI (Python).</li>
        <li><strong>Deployment</strong>: Render, Supabase, Upstash.</li>
      </ul>
    </section>

    <section id="phase-2" class="phase page-break">
      <h2>Phase 2: Modelling with Classes</h2>

      <h3>1. Class Diagrams</h3>
      <figure>
        <img src="./assets/class-diagram.png" alt="Class diagram" />
        <figcaption>Figure 2: Class Diagram</figcaption>
      </figure>

      <h3>2. Sequence Diagram</h3>
      <figure>
        <img src="./assets/sequence-submission-lifecycle.png" alt="Sequence diagram" />
        <figcaption>Figure 3: Sequence Diagram</figcaption>
      </figure>

      <h3>3. State Diagrams</h3>
      <figure>
        <img src="./assets/state-submission-session.png" alt="State diagram" />
        <figcaption>Figure 4: State Diagram</figcaption>
      </figure>

      <h3>4. Activity Diagrams</h3>
      <figure>
        <img src="./assets/activity-end-to-end.png" alt="Activity diagram" />
        <figcaption>Figure 5: Activity Diagram</figcaption>
      </figure>

      <h3>5. Component Diagrams</h3>
      <figure>
        <img src="./assets/component-architecture.png" alt="Component diagram" />
        <figcaption>Figure 6: Component Diagram</figcaption>
      </figure>

      <h3>6. Deployment Diagram</h3>
      <figure>
        <img src="./assets/deployment-render.png" alt="Deployment diagram" />
        <figcaption>Figure 7: Deployment Diagram</figcaption>
      </figure>
    </section>

    <section id="phase-3" class="phase page-break">
      <h2>Phase 3: User Interface</h2>

      <h3>1. Design User Interfaces</h3>
      <p>
        Implemented interfaces include login/signup, dashboard, challenge session, submission result,
        work experience, profile, and admin dashboard.
      </p>

      <h3>2. Demo the Prototype</h3>
      <p>
        Demonstration evidence covers authentication, challenge assignment, timed proctored session,
        submission lifecycle, result display, and admin monitoring workflows.
      </p>

      <h3>3. Evaluate User Interface</h3>
      <p>Heuristic evaluation was performed using Nielsen principles.</p>
      <table>
        <thead>
          <tr>
            <th>ID</th>
            <th>Heuristic</th>
            <th>Observed Defect</th>
            <th>Severity</th>
            <th>Recommendation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>H-01</td>
            <td>Visibility of status</td>
            <td>Queue/network/timer states split in some views</td>
            <td>Medium</td>
            <td>Unify operational status indicators</td>
          </tr>
          <tr>
            <td>H-02</td>
            <td>Consistency and standards</td>
            <td>Status labels vary between pages</td>
            <td>Low</td>
            <td>Standardize labels</td>
          </tr>
          <tr>
            <td>H-03</td>
            <td>Error prevention</td>
            <td>Category omission interrupts assignment request</td>
            <td>Low</td>
            <td>Require category before request</td>
          </tr>
        </tbody>
      </table>

      <h3>4. Validate Fields (Verification and Validation)</h3>
      <ul>
        <li>Authentication: email/password validation and role-bound access.</li>
        <li>Challenge submission: required challenge ID, language, and code payload.</li>
        <li>AI helper: validated session context and hint usage limits.</li>
        <li>Work experience: required core fields and URL validation for evidence links.</li>
        <li>Admin challenge setup: required category/seniority/duration with readiness checks.</li>
      </ul>
    </section>

    <section id="phase-4" class="phase page-break">
      <h2>Phase 4: Build the Database and Demonstrate Integration</h2>

      <h3>1. Build the Database</h3>
      <p>
        Database design is implemented in PostgreSQL and defined in
        <code>backend/db/schema.sql</code> with challenge, submission, run, proctoring,
        trust, and audit entities.
      </p>

      <h3>2. Manage Objects</h3>
      <p>
        Objects are managed through structured schema definitions, keys, foreign-key relationships,
        check constraints, and indexes for performance and integrity.
      </p>

      <h3>3. Normalization Process</h3>
      <p>
        The implemented model is normalized to 3NF by separating user-level, session-level,
        run-level, and per-test data entities.
      </p>

      <h3>4. Manipulate your data</h3>
      <p>Population and transaction scripts are provided in:</p>
      <ul>
        <li><code>academic-submission/sql/seed-academic-demo.sql</code></li>
        <li><code>academic-submission/sql/transactions.sql</code></li>
      </ul>

      <h3>5. Manage transaction</h3>
      <p>Transaction and reporting queries are provided in:</p>
      <ul>
        <li><code>academic-submission/sql/report-queries.sql</code></li>
        <li><code>academic-submission/reports/report-queries.sql</code></li>
      </ul>
      <p>These outputs correlate with the functional requirements and use cases defined in Phase 1.</p>
    </section>

    <section id="phase-5" class="phase page-break">
      <h2>Phase 5: Final Project Deliverance</h2>

      <h3>1. Application</h3>
      <p>
        YoScore is delivered as a fully working system with integrated user interface,
        process logic, and database management in a three-tier client-server architecture.
      </p>

      <h3>2. Document on Test cases and Test plan</h3>
      <ul>
        <li><code>academic-submission/testing/Test-Plan.md</code></li>
        <li><code>academic-submission/testing/Test-Cases.md</code></li>
        <li><code>academic-submission/testing/Test-Execution-Evidence.md</code></li>
      </ul>

      <h3>3. Reports</h3>
      <ol>
        <li>User trust summary report.</li>
        <li>Submission performance report.</li>
        <li>Proctoring violation report.</li>
        <li>Judge run health report.</li>
        <li>Work-experience verification/risk report.</li>
      </ol>

      <h3>4. Final Deliverable must include</h3>
      <ol>
        <li>Application deployment execution manual.</li>
        <li>Application archive with source code.</li>
        <li>Database backup and DDL script.</li>
        <li>Complete source code evidence.</li>
      </ol>
    </section>

    <section id="appendix" class="phase page-break">
      <h2>Appendix: Evidence and Source Code Proof</h2>

      <h3>A. Diagram Index</h3>
      <ul>
        <li>Figure 1: Use Case Diagram</li>
        <li>Figure 2: Class Diagram</li>
        <li>Figure 3: Sequence Diagram</li>
        <li>Figure 4: State Diagram</li>
        <li>Figure 5: Activity Diagram</li>
        <li>Figure 6: Component Diagram</li>
        <li>Figure 7: Deployment Diagram</li>
      </ul>

      <h3>B. SQL and Test Artifact Index</h3>
      <ul>
        <li><code>academic-submission/sql/seed-academic-demo.sql</code></li>
        <li><code>academic-submission/sql/transactions.sql</code></li>
        <li><code>academic-submission/sql/report-queries.sql</code></li>
        <li><code>academic-submission/testing/Test-Plan.md</code></li>
        <li><code>academic-submission/testing/Test-Cases.md</code></li>
        <li><code>academic-submission/testing/Test-Execution-Evidence.md</code></li>
      </ul>

      <h3>C. Source Code Proof (Single-PDF Requirement)</h3>
      <p><strong>Repository URL</strong>: <code>https://github.com/SthembisoGit/Yo-Score</code></p>
      <p><strong>Branch</strong>: <code>main</code></p>
      <p><strong>Reference Commit</strong>: <code>e893745a8797dd5a1a1503609e89cad09863cfea</code></p>
      <p><strong>Verification</strong>:</p>
      <ol>
        <li>Open the repository URL.</li>
        <li>Select branch <code>main</code>.</li>
        <li>Open commit <code>e893745a8797dd5a1a1503609e89cad09863cfea</code>.</li>
        <li>Confirm presence of frontend, backend, proctoring service, database schema, tests, and academic documents.</li>
      </ol>
    </section>
  </main>

  <footer class="document-footer">
    YoScore Outcome 7 Final Academic Submission (Single-PDF Version).
  </footer>
</body>
</html>
