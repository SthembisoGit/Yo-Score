<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>YoScore Academic Submission</title>
  <link rel="stylesheet" href="./styles.css" />
</head>
<body>
  <header class="document-header">
    <h1>YoScore: AI-Aware Developer Trust and Skill Scoring Platform</h1>
    <p class="subtitle">Industry Project Academic Submission (Phases 1-5)</p>
    <table class="meta-table">
      <tr><th>Student Name</th><td>________________________</td></tr>
      <tr><th>Student Number</th><td>________________________</td></tr>
      <tr><th>Qualification</th><td>Diploma in Computer Science</td></tr>
      <tr><th>Module</th><td>Industry Project (IP) - Business Analysis and Project Management</td></tr>
      <tr><th>Submission Date</th><td>________________________</td></tr>
    </table>
  </header>

  <nav class="toc" aria-label="Table of contents">
    <h2>Table of Contents</h2>
    <ol>
      <li><a href="#phase-1">Phase 1: Proposal</a></li>
      <li><a href="#phase-2">Phase 2: Modelling with Classes</a></li>
      <li><a href="#phase-3">Phase 3: User Interface</a></li>
      <li><a href="#phase-4">Phase 4: Build the Database and Demonstrate Integration</a></li>
      <li><a href="#phase-5">Phase 5: Final Project Deliverance</a></li>
      <li><a href="#appendix">Appendix</a></li>
    </ol>
  </nav>

  <main>
    <section id="phase-1" class="phase page-break">
      <h2>Phase 1: Proposal</h2>

      <h3>1. Name of the Project</h3>
      <p><strong>YoScore: AI-Aware Developer Trust and Skill Scoring Platform</strong></p>

      <h3>2. Domain Analysis</h3>
      <h4>2.1 General Field of Business</h4>
      <p>
        YoScore operates in software skills assessment and technical trust verification.
        It supports fair developer evaluation for hiring, training, and progression decisions.
      </p>

      <h4>2.2 Domain Terminology / Glossary</h4>
      <ul>
        <li><strong>Challenge</strong>: timed technical task aligned to category and level.</li>
        <li><strong>Category</strong>: track such as frontend, backend, cloud, security.</li>
        <li><strong>Seniority Band</strong>: Graduate (0-6), Junior (7-24), Mid (25-60), Senior (61+ months).</li>
        <li><strong>Submission</strong>: code attempt sent for judging.</li>
        <li><strong>Judge Run</strong>: isolated execution against test cases.</li>
        <li><strong>Proctoring Session</strong>: monitored challenge attempt.</li>
        <li><strong>Violation</strong>: suspicious behavior event.</li>
        <li><strong>AI Coach</strong>: constrained assistant (concept hints, no full solutions).</li>
        <li><strong>Trust Score</strong>: combined outcome from skills, behavior, and trusted experience.</li>
      </ul>

      <h4>2.3 Business Environment Understanding</h4>
      <p>
        AI coding assistants are now standard in developer workflows. The challenge is not to ban AI,
        but to verify understanding and practical capability under controlled conditions.
      </p>

      <h4>2.4 Current Tasks and Procedures</h4>
      <ul>
        <li>Candidates complete coding tests on different platforms with inconsistent controls.</li>
        <li>Recruiters manually interpret results and trust signals.</li>
        <li>Work experience is often self-declared with limited evidence checks.</li>
      </ul>

      <h4>2.5 Customers and Users</h4>
      <ul>
        <li><strong>Developers</strong>: prove practical skill and trustworthiness.</li>
        <li><strong>Admins</strong>: operate challenge, scoring, and integrity systems.</li>
        <li><strong>Recruiters/Educators</strong>: consume trust outcomes (portal expansion is future scope).</li>
      </ul>

      <h4>2.6 Competing Software</h4>
      <ul>
        <li>HackerRank</li>
        <li>Codility</li>
        <li>CodeSignal</li>
        <li>TestGorilla</li>
      </ul>

      <h4>2.7 Similarities to Other Domains</h4>
      <p>
        Similar to online examinations and e-learning competency systems where integrity, auditability,
        and consistent scoring are required.
      </p>

      <h3>3. Define the Problem</h3>
      <h4>3.1 Difficulty</h4>
      <p>
        Existing assessment flows do not consistently prove whether candidates can solve problems with understanding,
        especially when unrestricted AI or weak proctoring is involved.
      </p>
      <h4>3.2 Opportunity</h4>
      <p>
        Build a 3-tier client-server platform with judged scoring, proctoring, constrained AI coaching,
        and evidence-aware trust modeling.
      </p>

      <h3>4. Define the Scope (IRBM)</h3>
      <h4>4.1 Scope Boundary</h4>
      <p><strong>Release 1 in scope:</strong></p>
      <ul>
        <li>coding-only assessments (JavaScript and Python)</li>
        <li>category plus seniority assignment</li>
        <li>timed proctored sessions with offline continuity</li>
        <li>AI Coach with 3-hint policy</li>
        <li>work-experience evidence and risk status</li>
      </ul>
      <p><strong>Out of scope for Release 1:</strong></p>
      <ul>
        <li>mixed non-coding assessments (MCQ, explanation, scenarios)</li>
        <li>CV intelligence</li>
        <li>soft-skill tracking</li>
      </ul>

      <h4>4.2 Integrated Result Based Management (IRBM)</h4>
      <p><strong>Inputs</strong>: users, challenge bank, baselines, evidence links, proctoring policy, cloud infrastructure.</p>
      <p><strong>Activities</strong>: assign, monitor, judge, score, and audit.</p>
      <p><strong>Outputs</strong>: run outcomes, trust scores, violation logs, flagged queues.</p>
      <p><strong>Outcomes</strong>: stronger trust in assessment results and reduced manual review load.</p>
      <p><strong>Impact</strong>: fairer opportunities and better hiring signal quality.</p>

      <h4>4.3 Assess / Think / Envision / Plan</h4>
      <p><strong>Assess</strong>: trust in coding assessments is inconsistent.</p>
      <p><strong>Think</strong>: causes include unbounded AI usage and fragmented integrity controls.</p>
      <p><strong>Envision</strong>: a trusted AI-aware assessment system that rewards understanding.</p>
      <p><strong>Plan</strong>: deliver Trust-Core MVP in 2 days and defer mixed assessments to Release 1.1+.</p>

      <h3>5. Vision and Objectives (SMART)</h3>
      <h4>5.1 Vision</h4>
      <p>Measure not only what candidates submit, but how they solve, under transparent and auditable rules.</p>

      <h4>5.2 SMART Objectives</h4>
      <ol>
        <li>Deliver complete 3-tier Trust-Core MVP by deadline.</li>
        <li>Ensure all submissions use async judge lifecycle persistence.</li>
        <li>Enforce maximum 3 AI hints per challenge session.</li>
        <li>Enforce deadline plus 15-minute grace policy for offline reconnect submit.</li>
        <li>Route challenges by category with exact-then-lower seniority fallback.</li>
        <li>Store evidence/risk metadata on all new work-experience entries.</li>
      </ol>

      <h3>6. Users of the System</h3>
      <ul>
        <li><strong>Developer</strong>: challenge participation, AI hints, submission, dashboard review.</li>
        <li><strong>Admin</strong>: content and policy operations, queue and risk monitoring, role management.</li>
        <li><strong>Recruiter (limited in MVP)</strong>: downstream consumer of trust outputs.</li>
      </ul>

      <h3>7. Mandatory Functions (CRUD)</h3>
      <ul>
        <li>Add/register users.</li>
        <li>Add/update/delete challenges and test cases.</li>
        <li>Add/update baselines and reference docs.</li>
        <li>Add/list work experience with evidence links.</li>
        <li>Update persistent proctoring settings.</li>
      </ul>

      <h3>8. Functional Requirements</h3>
      <ol>
        <li>Category-based challenge request and seniority-aware assignment.</li>
        <li>Timer initialized by server deadline metadata.</li>
        <li>Offline autosave and reconnect auto-submit behavior.</li>
        <li>Two-phase proctoring: live telemetry plus async post-exam review.</li>
        <li>Submission queue lifecycle with run and test persistence.</li>
        <li>AI Coach policy enforcement (3 hints, no full solution output).</li>
        <li>Evidence-aware work-experience verification and risk scoring.</li>
        <li>Dashboard and admin views aligned to backend truth.</li>
      </ol>
      <p>
        Inputs include credentials, category, language, code, and evidence links.
        Outputs include scored results, trust summaries, and audit records.
      </p>

      <h3>9. Non-Functional Requirements</h3>
      <ul>
        <li><strong>Authentication</strong>: JWT login/logout and RBAC.</li>
        <li><strong>Availability</strong>: health endpoints and queue resilience.</li>
        <li><strong>Reliability</strong>: persistent run/proctoring/audit logs.</li>
        <li><strong>Security</strong>: password hashing, role checks, controlled CORS.</li>
        <li><strong>Maintainability</strong>: modular services with documented contracts.</li>
      </ul>

      <h3>10. Use Case</h3>
      <figure>
        <img src="./assets/use-case.png" alt="Use case diagram" />
        <figcaption>Figure 1: YoScore Use Case Diagram</figcaption>
      </figure>
      <p>
        Key use cases include category-based challenge start, proctored timed solving, constrained AI hint requests,
        async judged submission, and admin monitoring of flagged work-experience records.
      </p>

      <h3>11. Tools and Technologies to Be Used</h3>
      <ul>
        <li><strong>Product stack</strong>: React, TypeScript, Node.js, Express, BullMQ, PostgreSQL, Redis, FastAPI.</li>
        <li><strong>Deployment</strong>: Render + Supabase + Upstash.</li>
        <li><strong>Academic tooling</strong>: PlantUML, Kroki, diagrams.net, LibreOffice, OBS Studio.</li>
      </ul>
    </section>

    <section id="phase-2" class="phase page-break">
      <h2>Phase 2: Modelling with Classes</h2>

      <h3>1. Class Diagram</h3>
      <figure>
        <img src="./assets/class-diagram.png" alt="Class diagram" />
        <figcaption>Figure 2: Class Diagram</figcaption>
      </figure>

      <h3>2. Sequence Diagram</h3>
      <figure>
        <img src="./assets/sequence-submission-lifecycle.png" alt="Sequence diagram" />
        <figcaption>Figure 3: Submission Lifecycle Sequence Diagram</figcaption>
      </figure>

      <h3>3. State Diagram</h3>
      <figure>
        <img src="./assets/state-submission-session.png" alt="State diagram" />
        <figcaption>Figure 4: Submission and Session State Diagram</figcaption>
      </figure>

      <h3>4. Activity Diagram</h3>
      <figure>
        <img src="./assets/activity-end-to-end.png" alt="Activity diagram" />
        <figcaption>Figure 5: End-to-End Activity Diagram</figcaption>
      </figure>

      <h3>5. Component Diagram</h3>
      <figure>
        <img src="./assets/component-architecture.png" alt="Component diagram" />
        <figcaption>Figure 6: Component Diagram</figcaption>
      </figure>

      <h3>6. Deployment Diagram</h3>
      <figure>
        <img src="./assets/deployment-render.png" alt="Deployment diagram" />
        <figcaption>Figure 7: Deployment Diagram</figcaption>
      </figure>
    </section>

    <section id="phase-3" class="phase page-break">
      <h2>Phase 3: User Interface</h2>

      <h3>1. Design User Interfaces</h3>
      <p>Implemented screens include login, dashboard, challenge session, result view, work-experience view, profile page, and admin dashboard.</p>
      <ul>
        <li>Category picker and seniority badge in challenge flow</li>
        <li>Server-based countdown timer</li>
        <li>Offline banner plus autosave/reconnect behavior</li>
        <li>AI Coach panel with remaining hint counter</li>
        <li>Work-experience evidence links and risk status badges</li>
        <li>Profile editing for avatar URL, bio, and professional links</li>
      </ul>

      <h3>2. Demo the Prototype</h3>
      <ol>
        <li>Login as developer.</li>
        <li>Select category and get assigned challenge.</li>
        <li>Start session and verify timer.</li>
        <li>Request AI Coach hint.</li>
        <li>Submit and observe judge lifecycle.</li>
        <li>Review updated dashboard.</li>
        <li>Login as admin and inspect flagged experience queue.</li>
        <li>Open session detail and verify post-exam proctoring summary.</li>
      </ol>

      <h3>3. Evaluate User Interface (Heuristic Evaluation)</h3>
      <table>
        <thead>
          <tr>
            <th>ID</th>
            <th>Heuristic</th>
            <th>Defect</th>
            <th>Severity</th>
            <th>Recommendation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>H-01</td>
            <td>Visibility of status</td>
            <td>Timer/network/judge states are split across areas</td>
            <td>Medium</td>
            <td>Use one consolidated status strip</td>
          </tr>
          <tr>
            <td>H-02</td>
            <td>Consistency</td>
            <td>Status labels vary between pages</td>
            <td>Low</td>
            <td>Standardize labels globally</td>
          </tr>
          <tr>
            <td>H-03</td>
            <td>Error prevention</td>
            <td>AI Coach constraints are not always obvious</td>
            <td>Medium</td>
            <td>Show policy text in panel header</td>
          </tr>
          <tr>
            <td>H-04</td>
            <td>Recognition over recall</td>
            <td>Risk status meaning can be unclear</td>
            <td>Low</td>
            <td>Add tooltip definitions for pending/flagged/verified</td>
          </tr>
        </tbody>
      </table>

      <h3>4. Validate Fields (Verification and Validation)</h3>
      <ul>
        <li>Authentication: valid email and required password.</li>
        <li>Submission: required challenge id, code, and language (`javascript` or `python`).</li>
        <li>AI Coach: hard 3-hint cap per challenge session.</li>
        <li>Work experience: required company, role, positive duration, optional URL evidence list.</li>
        <li>Admin challenge setup: required seniority and bounded duration.</li>
      </ul>
    </section>

    <section id="phase-4" class="phase page-break">
      <h2>Phase 4: Build the Database and Demonstrate Integration</h2>

      <h3>1. Build the Database</h3>
      <p>
        The PostgreSQL schema is defined in <code>backend/db/schema.sql</code> and includes challenge,
        submission, run-level, proctoring (live and post-exam evidence), trust, work-experience, and AI hint audit entities.
      </p>

      <h3>2. Manage Objects</h3>
      <ul>
        <li>Primary and foreign keys enforce relational integrity.</li>
        <li>Check constraints enforce language, status, and verification enums.</li>
        <li>Indexes support lifecycle polling, assignment, and risk auditing.</li>
        <li>Two-phase proctoring evidence is stored in <code>proctoring_event_logs</code> and <code>proctoring_snapshots</code>.</li>
      </ul>

      <h3>3. Normalization Process</h3>
      <p>The model is normalized to 3NF by separating user, submission, run, and per-test data.</p>
      <pre><code>Before (not used): assessment_blob(user, challenge, tests_json, score)
After: submissions + submission_runs + submission_run_tests + trust_scores</code></pre>

      <h3>4. Manipulate Data</h3>
      <p>Academic SQL scripts:</p>
      <ul>
        <li><code>academic-submission/sql/seed-academic-demo.sql</code></li>
        <li><code>academic-submission/sql/transactions.sql</code></li>
      </ul>

      <h3>5. Manage Transactions and Queries</h3>
      <p>Report queries are provided in:</p>
      <ul>
        <li><code>academic-submission/sql/report-queries.sql</code></li>
        <li><code>academic-submission/reports/report-queries.sql</code></li>
      </ul>
      <p>These cover trust summaries, performance outputs, proctoring analytics, and judge health data.</p>
    </section>

    <section id="phase-5" class="phase page-break">
      <h2>Phase 5: Final Project Deliverance</h2>

      <h3>1. Application</h3>
      <p>
        YoScore is a complete integrated 3-tier application with frontend, backend process logic,
        and database services connected through REST and queue workflows.
      </p>

      <h3>2. Document on Test Cases and Test Plan</h3>
      <ul>
        <li><code>academic-submission/testing/Test-Plan.md</code></li>
        <li><code>academic-submission/testing/Test-Cases.md</code></li>
        <li><code>academic-submission/testing/Test-Execution-Evidence.md</code></li>
      </ul>

      <h3>3. Reports</h3>
      <ol>
        <li>User trust summary report</li>
        <li>Submission performance report</li>
        <li>Proctoring violation report</li>
        <li>Judge run health report</li>
        <li>Flagged work-experience audit report</li>
      </ol>

      <h3>4. Final Deliverable Must Include</h3>
      <ol>
        <li>Application deployment execution manual</li>
        <li>Application source archive</li>
        <li>Database backup and DDL script</li>
        <li>Test plan, test cases, and execution evidence</li>
        <li>Diagram source files and exported images</li>
        <li>Prototype demo recording</li>
        <li>DOCX and PDF report outputs</li>
      </ol>
    </section>

    <section id="appendix" class="phase page-break">
      <h2>Appendix</h2>
      <ul>
        <li>Phase docs: <code>academic-submission/Phase-1-Proposal.md</code> to <code>academic-submission/Phase-5-Final-Deliverables.md</code></li>
        <li>Diagram sources: <code>academic-submission/diagrams/*.puml</code></li>
        <li>Diagram exports: <code>academic-submission/diagrams/exports/*.png</code></li>
        <li>SQL scripts: <code>academic-submission/sql/*.sql</code></li>
        <li>Report SQL: <code>academic-submission/reports/report-queries.sql</code></li>
      </ul>
    </section>
  </main>

  <footer class="document-footer">
    YoScore Academic Submission - Revision aligned to Trust-Core MVP and Release 1 roadmap.
  </footer>
</body>
</html>
